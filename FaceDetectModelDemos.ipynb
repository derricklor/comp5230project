{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57c5775d",
   "metadata": {},
   "source": [
    "<h1> Build face detection model from multiple datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1348fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.107 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.105  Python-3.10.6 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 4070, 12281MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\multiple_datasets.yaml, epochs=3, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train19, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train19\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mUltralytics HUB: \u001b[0mView model at https://hub.ultralytics.com/models/0yLKHK5QvUxFfkjaku8b \n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train19', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\datasets\\FaceDetect-5\\train\\labels... 3898 images, 0 backgrounds, 0 corrupt: 100%|██████████| 3898/3898 [00:01<00:00, 1990.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\datasets\\FaceDetect-5\\train\\labels.cache\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 18, len(boxes) = 3906. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\datasets\\FaceDetect-5\\valid\\labels... 413 images, 0 backgrounds, 0 corrupt: 100%|██████████| 413/413 [00:00<00:00, 1119.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\datasets\\FaceDetect-5\\valid\\labels.cache\n",
      "WARNING  Box and segment counts should be equal, but got len(segments) = 3, len(boxes) = 417. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
      "Plotting labels to runs\\detect\\train19\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 6 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train19\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/3      2.35G      1.111      1.385      1.519         32        640: 100%|██████████| 244/244 [00:47<00:00,  5.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:03<00:00,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417      0.958       0.93      0.971      0.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/3      2.69G     0.9893     0.7279      1.384         23        640: 100%|██████████| 244/244 [00:39<00:00,  6.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:02<00:00,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417      0.954      0.894      0.965      0.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/3       2.7G     0.8745     0.5544      1.294         25        640: 100%|██████████| 244/244 [00:38<00:00,  6.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:02<00:00,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417      0.959      0.957      0.989      0.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 epochs completed in 0.039 hours.\n",
      "Optimizer stripped from runs\\detect\\train19\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train19\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train19\\weights\\best.pt...\n",
      "Ultralytics 8.3.105  Python-3.10.6 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 4070, 12281MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:03<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417      0.959      0.957      0.989      0.555\n",
      "Speed: 0.2ms preprocess, 1.6ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train19\u001b[0m\n",
      "\u001b[34m\u001b[1mUltralytics HUB: \u001b[0mSyncing final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.19M/5.19M [00:00<00:00, 11.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mUltralytics HUB: \u001b[0mDone \n",
      "\u001b[34m\u001b[1mUltralytics HUB: \u001b[0mView model at https://hub.ultralytics.com/models/0yLKHK5QvUxFfkjaku8b \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO(\"yolo11n.yaml\")  # build a new model from YAML\n",
    "model = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n",
    "#model = YOLO(\"yolo11n.yaml\").load(\"yolo11n.pt\")  # build from YAML and transfer weights\n",
    "\n",
    "# Train the model\n",
    "single_dataset = \"C:\\\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\datasets\\FaceDetect-5\\data.yaml\"\n",
    "multi_datasets = \"C:\\\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\multiple_datasets.yaml\"\n",
    "results = model.train(data=multi_datasets, epochs=3, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea7e8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.114 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.105  Python-3.10.6 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 4070, 12281MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\runs\\detect\\train192\\weights\\best.pt, data=C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\multiple_datasets.yaml, epochs=13, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train20, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train20\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mUltralytics HUB: \u001b[0mView model at https://hub.ultralytics.com/models/8b6KTRlNWclsF11cr9LE \n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train20', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\datasets\\FaceDetect-5\\train\\labels.cache... 3898 images, 0 backgrounds, 0 corrupt: 100%|██████████| 3898/3898 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Box and segment counts should be equal, but got len(segments) = 18, len(boxes) = 3906. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\datasets\\FaceDetect-5\\valid\\labels.cache... 413 images, 0 backgrounds, 0 corrupt: 100%|██████████| 413/413 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Box and segment counts should be equal, but got len(segments) = 3, len(boxes) = 417. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train20\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 6 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train20\u001b[0m\n",
      "Starting training for 13 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/13      2.35G     0.6776     0.3577      1.155         32        640: 100%|██████████| 244/244 [00:54<00:00,  4.52it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:05<00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417      0.983      0.969      0.993      0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/13      2.69G     0.7607     0.4225      1.209         23        640: 100%|██████████| 244/244 [00:48<00:00,  5.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:03<00:00,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417      0.966      0.946      0.989      0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/13      2.71G     0.8075     0.4474      1.243         25        640: 100%|██████████| 244/244 [00:45<00:00,  5.42it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:02<00:00,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417      0.976      0.978      0.989      0.572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/13      2.71G     0.7837     0.3858      1.392         10        640: 100%|██████████| 244/244 [00:39<00:00,  6.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:02<00:00,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417      0.933       0.89      0.941      0.541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/13      2.71G     0.7413     0.3531      1.355         10        640: 100%|██████████| 244/244 [00:40<00:00,  6.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:03<00:00,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417      0.931      0.914      0.969      0.545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/13      2.71G     0.7134     0.3337      1.323         10        640: 100%|██████████| 244/244 [00:40<00:00,  6.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:02<00:00,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417      0.993      0.952      0.985      0.545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/13      2.71G     0.6697     0.3084      1.262         10        640: 100%|██████████| 244/244 [00:40<00:00,  6.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:02<00:00,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417      0.978      0.986      0.994      0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/13      2.73G     0.6419     0.2907      1.235         10        640: 100%|██████████| 244/244 [00:40<00:00,  5.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:02<00:00,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417       0.99       0.99      0.995      0.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/13      2.74G     0.6228     0.2788      1.223         10        640: 100%|██████████| 244/244 [00:39<00:00,  6.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:02<00:00,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417      0.981      0.983      0.993      0.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/13      2.74G     0.6029     0.2676      1.208         10        640: 100%|██████████| 244/244 [00:39<00:00,  6.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:03<00:00,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417      0.998       0.99      0.995      0.611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/13      2.74G     0.5847     0.2526      1.183         10        640: 100%|██████████| 244/244 [00:49<00:00,  4.94it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:03<00:00,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417      0.995       0.99      0.995      0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/13      2.74G     0.5495     0.2361      1.158         10        640: 100%|██████████| 244/244 [00:49<00:00,  4.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:03<00:00,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417          1      0.987      0.995      0.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/13      2.74G     0.5289     0.2228      1.133         10        640: 100%|██████████| 244/244 [00:44<00:00,  5.51it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:03<00:00,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417          1      0.993      0.995      0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13 epochs completed in 0.180 hours.\n",
      "Optimizer stripped from runs\\detect\\train20\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train20\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train20\\weights\\best.pt...\n",
      "Ultralytics 8.3.105  Python-3.10.6 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 4070, 12281MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 13/13 [00:03<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417          1      0.993      0.995      0.644\n",
      "Speed: 0.2ms preprocess, 0.9ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train20\u001b[0m\n",
      "\u001b[34m\u001b[1mUltralytics HUB: \u001b[0mSyncing final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.20M/5.20M [00:00<00:00, 11.4MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mUltralytics HUB: \u001b[0mDone \n",
      "\u001b[34m\u001b[1mUltralytics HUB: \u001b[0mView model at https://hub.ultralytics.com/models/8b6KTRlNWclsF11cr9LE \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#train model 192 for more epochs to get the mAP higher, keeping a checkpoint to ever revert back\n",
    "multi_datasets = \"C:\\\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\multiple_datasets.yaml\"\n",
    "model = YOLO(r\"C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\runs\\detect\\train192\\weights\\best.pt\")\n",
    "# train same model again for more epochs\n",
    "results = model.train(data=multi_datasets, epochs=13, imgsz=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870abb71",
   "metadata": {},
   "source": [
    "<h1> Build face detection model from single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c94f6bd-6a4d-4107-af16-9ef8ebcf5b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt to 'yolo11m.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38.8M/38.8M [00:02<00:00, 14.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.107 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.105  Python-3.10.6 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 4070, 12281MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11m.pt, data=C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\datasets\\FaceDetect-5\\data.yaml, epochs=3, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train18, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train18\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   1411795  ultralytics.nn.modules.head.Detect           [1, [256, 512, 512]]          \n",
      "YOLO11m summary: 231 layers, 20,053,779 parameters, 20,053,763 gradients, 68.2 GFLOPs\n",
      "\n",
      "Transferred 643/649 items from pretrained weights\n",
      "\u001b[34m\u001b[1mUltralytics HUB: \u001b[0mView model at https://hub.ultralytics.com/models/rdGPXJsRlyTWFgMmzzlc \n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train18', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\datasets\\FaceDetect-5\\train\\labels.cache... 2399 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2399/2399 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\datasets\\FaceDetect-5\\valid\\labels.cache... 100 images, 0 backgrounds, 0 corrupt: 100%|██████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train18\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 106 weight(decay=0.0), 113 weight(decay=0.0005), 112 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 6 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train18\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/3      8.02G      1.117     0.8555      1.586         42        640: 100%|██████████| 150/150 [00:46<00:00,  3.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        100        100    0.00564       0.42    0.00419   0.000646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/3      9.51G     0.9702     0.5943      1.436         31        640: 100%|██████████| 150/150 [00:42<00:00,  3.57it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00<00:00,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        100        100      0.139       0.46     0.0966     0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/3      9.58G     0.8091     0.4836      1.309         48        640: 100%|██████████| 150/150 [00:41<00:00,  3.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00<00:00,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        100        100       0.98      0.981      0.993      0.373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 epochs completed in 0.040 hours.\n",
      "Optimizer stripped from runs\\detect\\train18\\weights\\last.pt, 40.5MB\n",
      "Optimizer stripped from runs\\detect\\train18\\weights\\best.pt, 40.5MB\n",
      "\n",
      "Validating runs\\detect\\train18\\weights\\best.pt...\n",
      "Ultralytics 8.3.105  Python-3.10.6 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 4070, 12281MiB)\n",
      "YOLO11m summary (fused): 125 layers, 20,030,803 parameters, 0 gradients, 67.6 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        100        100       0.98      0.981      0.993      0.373\n",
      "Speed: 0.2ms preprocess, 5.4ms inference, 0.0ms loss, 2.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train18\u001b[0m\n",
      "\u001b[34m\u001b[1mUltralytics HUB: \u001b[0mSyncing final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38.6M/38.6M [00:03<00:00, 13.0MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mUltralytics HUB: \u001b[0mDone \n",
      "\u001b[34m\u001b[1mUltralytics HUB: \u001b[0mView model at https://hub.ultralytics.com/models/rdGPXJsRlyTWFgMmzzlc \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "#model = YOLO(\"yolo11n.yaml\")  # build a new model from YAML\n",
    "model = YOLO(\"yolo11m.pt\")  # load a pretrained model (recommended for training)\n",
    "#model = YOLO(\"yolo11n.yaml\").load(\"yolo11n.pt\")  # build from YAML and transfer weights\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=\"C:\\\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\datasets\\FaceDetect-5\\data.yaml\", epochs=3, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "266edff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.107 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.105  Python-3.10.6 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 4070, 12281MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11m.pt, data=C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\datasets\\FaceDetect-5\\data.yaml, epochs=7, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train182, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train182\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   1411795  ultralytics.nn.modules.head.Detect           [1, [256, 512, 512]]          \n",
      "YOLO11m summary: 231 layers, 20,053,779 parameters, 20,053,763 gradients, 68.2 GFLOPs\n",
      "\n",
      "Transferred 649/649 items from pretrained weights\n",
      "\u001b[34m\u001b[1mUltralytics HUB: \u001b[0mView model at https://hub.ultralytics.com/models/JvcxGOXIuwtcfZYeVLbe \n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train182', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\datasets\\FaceDetect-5\\train\\labels.cache... 2399 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2399/2399 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\datasets\\FaceDetect-5\\valid\\labels.cache... 100 images, 0 backgrounds, 0 corrupt: 100%|██████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train182\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 106 weight(decay=0.0), 113 weight(decay=0.0005), 112 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 6 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train182\u001b[0m\n",
      "Starting training for 7 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/7      8.08G     0.7427     0.4371      1.262         42        640: 100%|██████████| 150/150 [00:46<00:00,  3.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        100        100      0.867       0.94      0.965      0.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/7      9.56G     0.8193      0.482      1.315         31        640: 100%|██████████| 150/150 [00:42<00:00,  3.51it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        100        100      0.839       0.91      0.924      0.308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/7      9.62G     0.7706     0.4601      1.279         48        640: 100%|██████████| 150/150 [00:42<00:00,  3.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00<00:00,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        100        100       0.96      0.968      0.988      0.422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/7      9.69G     0.7225     0.4229      1.238         34        640: 100%|██████████| 150/150 [00:41<00:00,  3.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00<00:00,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        100        100      0.977       0.97      0.984      0.473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/7      9.69G     0.6572     0.3932      1.194         28        640: 100%|██████████| 150/150 [00:40<00:00,  3.67it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00<00:00,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        100        100      0.956       0.96      0.986      0.495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        6/7      9.69G     0.5872     0.3464      1.147         45        640: 100%|██████████| 150/150 [00:41<00:00,  3.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00<00:00,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        100        100      0.999          1      0.995      0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        7/7      9.69G      0.519     0.3068      1.105         36        640: 100%|██████████| 150/150 [00:41<00:00,  3.63it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:00<00:00,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        100        100      0.989          1      0.995      0.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7 epochs completed in 0.088 hours.\n",
      "Optimizer stripped from runs\\detect\\train182\\weights\\last.pt, 40.5MB\n",
      "Optimizer stripped from runs\\detect\\train182\\weights\\best.pt, 40.5MB\n",
      "\n",
      "Validating runs\\detect\\train182\\weights\\best.pt...\n",
      "Ultralytics 8.3.105  Python-3.10.6 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 4070, 12281MiB)\n",
      "YOLO11m summary (fused): 125 layers, 20,030,803 parameters, 0 gradients, 67.6 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:01<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        100        100      0.989          1      0.995      0.555\n",
      "Speed: 0.2ms preprocess, 5.8ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train182\u001b[0m\n",
      "\u001b[34m\u001b[1mUltralytics HUB: \u001b[0mSyncing final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38.6M/38.6M [00:02<00:00, 13.7MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mUltralytics HUB: \u001b[0mDone \n",
      "\u001b[34m\u001b[1mUltralytics HUB: \u001b[0mView model at https://hub.ultralytics.com/models/JvcxGOXIuwtcfZYeVLbe \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train some more\n",
    "results = model.train(data=\"C:\\\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\datasets\\FaceDetect-5\\data.yaml\", epochs=7, imgsz=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3774ded9",
   "metadata": {},
   "source": [
    "<h1> Load the model and validate it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65bcb40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84852b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.105  Python-3.10.6 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 4070, 12281MiB)\n",
      "YOLO11n summary (fused): 100 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\datasets\\FaceDetect-5\\valid\\labels.cache... 413 images, 0 backgrounds, 0 corrupt: 100%|██████████| 413/413 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Box and segment counts should be equal, but got len(segments) = 3, len(boxes) = 417. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 26/26 [00:04<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        413        417          1      0.993      0.995      0.645\n",
      "Speed: 0.5ms preprocess, 3.5ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val4\u001b[0m\n",
      "0.6451909766457657\n"
     ]
    }
   ],
   "source": [
    "# Load the YOLO11 model\n",
    "model = YOLO(\"yolo11n_FaceDetectModel_MD20.pt\")\n",
    "\n",
    "# Validate the model\n",
    "metrics = model.val()\n",
    "print(metrics.box.map)  # mAP50-95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e56d8b5",
   "metadata": {},
   "source": [
    "<h1> Demo detect faces and compare from directory of whitelist and blacklist face images with face alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a569e23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 57.0ms\n",
      "Speed: 3.2ms preprocess, 57.0ms inference, 15.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.0ms\n",
      "Speed: 1.6ms preprocess, 12.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.0ms\n",
      "Speed: 1.1ms preprocess, 12.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.7ms\n",
      "Speed: 1.2ms preprocess, 11.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.9ms\n",
      "Speed: 1.3ms preprocess, 11.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.1ms\n",
      "Speed: 1.1ms preprocess, 12.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.3ms\n",
      "Speed: 1.1ms preprocess, 14.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.4ms\n",
      "Speed: 1.4ms preprocess, 14.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.9ms\n",
      "Speed: 1.1ms preprocess, 11.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.4ms\n",
      "Speed: 1.2ms preprocess, 12.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.0ms\n",
      "Speed: 1.3ms preprocess, 13.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.8ms\n",
      "Speed: 2.7ms preprocess, 15.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.5ms\n",
      "Speed: 1.1ms preprocess, 12.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.7ms\n",
      "Speed: 1.2ms preprocess, 12.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.2ms\n",
      "Speed: 1.5ms preprocess, 17.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.3ms\n",
      "Speed: 1.1ms preprocess, 12.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.9ms\n",
      "Speed: 1.3ms preprocess, 12.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.8ms\n",
      "Speed: 1.4ms preprocess, 11.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.0ms\n",
      "Speed: 1.1ms preprocess, 12.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.0ms\n",
      "Speed: 1.2ms preprocess, 12.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.4ms\n",
      "Speed: 1.1ms preprocess, 12.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.7ms\n",
      "Speed: 1.2ms preprocess, 11.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.7ms\n",
      "Speed: 1.1ms preprocess, 11.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.4ms\n",
      "Speed: 1.6ms preprocess, 13.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.8ms\n",
      "Speed: 1.3ms preprocess, 11.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.2ms\n",
      "Speed: 1.1ms preprocess, 12.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.7ms\n",
      "Speed: 1.1ms preprocess, 13.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.6ms\n",
      "Speed: 1.3ms preprocess, 12.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.6ms\n",
      "Speed: 1.2ms preprocess, 12.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.0ms\n",
      "Speed: 1.2ms preprocess, 16.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.6ms\n",
      "Speed: 1.1ms preprocess, 14.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.8ms\n",
      "Speed: 1.5ms preprocess, 17.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.9ms\n",
      "Speed: 1.2ms preprocess, 12.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.1ms\n",
      "Speed: 1.5ms preprocess, 13.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.4ms\n",
      "Speed: 1.3ms preprocess, 12.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.6ms\n",
      "Speed: 1.3ms preprocess, 13.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.3ms\n",
      "Speed: 1.1ms preprocess, 15.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.7ms\n",
      "Speed: 1.1ms preprocess, 12.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.2ms\n",
      "Speed: 1.1ms preprocess, 14.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.7ms\n",
      "Speed: 1.5ms preprocess, 15.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.0ms\n",
      "Speed: 1.7ms preprocess, 13.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.5ms\n",
      "Speed: 2.2ms preprocess, 19.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 34.9ms\n",
      "Speed: 3.9ms preprocess, 34.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 39.7ms\n",
      "Speed: 3.5ms preprocess, 39.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 29.1ms\n",
      "Speed: 2.1ms preprocess, 29.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 36.0ms\n",
      "Speed: 4.3ms preprocess, 36.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 35.5ms\n",
      "Speed: 3.9ms preprocess, 35.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 22.6ms\n",
      "Speed: 3.4ms preprocess, 22.6ms inference, 61.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.4ms\n",
      "Speed: 2.5ms preprocess, 14.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.7ms\n",
      "Speed: 1.6ms preprocess, 12.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.0ms\n",
      "Speed: 1.9ms preprocess, 16.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.5ms\n",
      "Speed: 1.8ms preprocess, 14.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.0ms\n",
      "Speed: 1.5ms preprocess, 15.0ms inference, 4.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 21.4ms\n",
      "Speed: 3.0ms preprocess, 21.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.3ms\n",
      "Speed: 4.7ms preprocess, 13.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.8ms\n",
      "Speed: 2.1ms preprocess, 19.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.9ms\n",
      "Speed: 1.3ms preprocess, 15.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.9ms\n",
      "Speed: 1.3ms preprocess, 19.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.0ms\n",
      "Speed: 1.4ms preprocess, 14.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 23.0ms\n",
      "Speed: 2.6ms preprocess, 23.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 35.7ms\n",
      "Speed: 4.6ms preprocess, 35.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 35.3ms\n",
      "Speed: 4.6ms preprocess, 35.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 21.3ms\n",
      "Speed: 2.2ms preprocess, 21.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 12.0ms\n",
      "Speed: 1.6ms preprocess, 12.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 18.2ms\n",
      "Speed: 3.3ms preprocess, 18.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 8.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 15.6ms\n",
      "Speed: 1.7ms preprocess, 15.6ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 12.2ms\n",
      "Speed: 2.0ms preprocess, 12.2ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 11.7ms\n",
      "Speed: 1.5ms preprocess, 11.7ms inference, 9.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 15.4ms\n",
      "Speed: 2.3ms preprocess, 15.4ms inference, 4.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 12.6ms\n",
      "Speed: 2.3ms preprocess, 12.6ms inference, 8.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 19.1ms\n",
      "Speed: 4.7ms preprocess, 19.1ms inference, 6.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 13.8ms\n",
      "Speed: 2.2ms preprocess, 13.8ms inference, 5.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 13.8ms\n",
      "Speed: 2.8ms preprocess, 13.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 16.9ms\n",
      "Speed: 3.2ms preprocess, 16.9ms inference, 6.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.5ms\n",
      "Speed: 1.7ms preprocess, 14.5ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 25.8ms\n",
      "Speed: 4.4ms preprocess, 25.8ms inference, 9.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 25.3ms\n",
      "Speed: 3.8ms preprocess, 25.3ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 18.8ms\n",
      "Speed: 2.3ms preprocess, 18.8ms inference, 6.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 19.1ms\n",
      "Speed: 2.1ms preprocess, 19.1ms inference, 5.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 23.9ms\n",
      "Speed: 2.0ms preprocess, 23.9ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 16.1ms\n",
      "Speed: 2.0ms preprocess, 16.1ms inference, 7.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 19.0ms\n",
      "Speed: 1.8ms preprocess, 19.0ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 13.4ms\n",
      "Speed: 1.8ms preprocess, 13.4ms inference, 8.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.2ms\n",
      "Speed: 2.0ms preprocess, 17.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.8ms\n",
      "Speed: 1.6ms preprocess, 17.8ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.6ms\n",
      "Speed: 1.7ms preprocess, 13.6ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.6ms\n",
      "Speed: 1.8ms preprocess, 19.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.1ms\n",
      "Speed: 2.5ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.1ms\n",
      "Speed: 2.3ms preprocess, 13.1ms inference, 4.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.1ms\n",
      "Speed: 2.3ms preprocess, 15.1ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.6ms\n",
      "Speed: 1.9ms preprocess, 16.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.8ms\n",
      "Speed: 2.6ms preprocess, 17.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 13.1ms\n",
      "Speed: 1.9ms preprocess, 13.1ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.2ms\n",
      "Speed: 1.9ms preprocess, 19.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.0ms\n",
      "Speed: 1.9ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.7ms\n",
      "Speed: 2.0ms preprocess, 14.7ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.7ms\n",
      "Speed: 2.1ms preprocess, 19.7ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.6ms\n",
      "Speed: 2.0ms preprocess, 14.6ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.5ms\n",
      "Speed: 2.0ms preprocess, 14.5ms inference, 4.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.7ms\n",
      "Speed: 2.1ms preprocess, 19.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 22.4ms\n",
      "Speed: 2.2ms preprocess, 22.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.1ms\n",
      "Speed: 1.8ms preprocess, 15.1ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.6ms\n",
      "Speed: 1.8ms preprocess, 14.6ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 27.6ms\n",
      "Speed: 2.9ms preprocess, 27.6ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.6ms\n",
      "Speed: 1.9ms preprocess, 19.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.6ms\n",
      "Speed: 2.0ms preprocess, 17.6ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.2ms\n",
      "Speed: 1.8ms preprocess, 18.2ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.4ms\n",
      "Speed: 2.2ms preprocess, 13.4ms inference, 4.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.2ms\n",
      "Speed: 2.2ms preprocess, 16.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.4ms\n",
      "Speed: 1.7ms preprocess, 13.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.2ms\n",
      "Speed: 1.9ms preprocess, 18.2ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.5ms\n",
      "Speed: 3.4ms preprocess, 16.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.8ms\n",
      "Speed: 2.2ms preprocess, 12.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.7ms\n",
      "Speed: 1.9ms preprocess, 19.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 13.8ms\n",
      "Speed: 2.0ms preprocess, 13.8ms inference, 6.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 21.7ms\n",
      "Speed: 3.2ms preprocess, 21.7ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 24.8ms\n",
      "Speed: 2.3ms preprocess, 24.8ms inference, 7.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 15.7ms\n",
      "Speed: 1.7ms preprocess, 15.7ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 14.8ms\n",
      "Speed: 2.0ms preprocess, 14.8ms inference, 9.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 19.1ms\n",
      "Speed: 1.8ms preprocess, 19.1ms inference, 6.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 19.5ms\n",
      "Speed: 2.0ms preprocess, 19.5ms inference, 5.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 15.2ms\n",
      "Speed: 2.9ms preprocess, 15.2ms inference, 4.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 14.1ms\n",
      "Speed: 1.8ms preprocess, 14.1ms inference, 7.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 14.0ms\n",
      "Speed: 1.7ms preprocess, 14.0ms inference, 9.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 15.1ms\n",
      "Speed: 2.0ms preprocess, 15.1ms inference, 10.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 14.6ms\n",
      "Speed: 2.5ms preprocess, 14.6ms inference, 7.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 23.5ms\n",
      "Speed: 3.0ms preprocess, 23.5ms inference, 7.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 20.5ms\n",
      "Speed: 2.2ms preprocess, 20.5ms inference, 4.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.6ms\n",
      "Speed: 1.8ms preprocess, 19.6ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.5ms\n",
      "Speed: 2.6ms preprocess, 16.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.4ms\n",
      "Speed: 2.4ms preprocess, 16.4ms inference, 4.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.8ms\n",
      "Speed: 1.8ms preprocess, 19.8ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.7ms\n",
      "Speed: 2.4ms preprocess, 19.7ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.5ms\n",
      "Speed: 3.4ms preprocess, 16.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.2ms\n",
      "Speed: 4.1ms preprocess, 19.2ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.2ms\n",
      "Speed: 1.9ms preprocess, 18.2ms inference, 4.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.1ms\n",
      "Speed: 1.9ms preprocess, 15.1ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.6ms\n",
      "Speed: 2.1ms preprocess, 16.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.7ms\n",
      "Speed: 3.5ms preprocess, 14.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 20.4ms\n",
      "Speed: 2.1ms preprocess, 20.4ms inference, 5.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.5ms\n",
      "Speed: 2.7ms preprocess, 16.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 20.2ms\n",
      "Speed: 2.1ms preprocess, 20.2ms inference, 4.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 20.1ms\n",
      "Speed: 2.6ms preprocess, 20.1ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.4ms\n",
      "Speed: 1.7ms preprocess, 13.4ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.7ms\n",
      "Speed: 4.2ms preprocess, 18.7ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 23.2ms\n",
      "Speed: 4.1ms preprocess, 23.2ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.2ms\n",
      "Speed: 1.7ms preprocess, 13.2ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.7ms\n",
      "Speed: 1.8ms preprocess, 13.7ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.0ms\n",
      "Speed: 2.2ms preprocess, 13.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.7ms\n",
      "Speed: 2.2ms preprocess, 13.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.0ms\n",
      "Speed: 1.6ms preprocess, 13.0ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.5ms\n",
      "Speed: 2.1ms preprocess, 17.5ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.1ms\n",
      "Speed: 2.5ms preprocess, 16.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.7ms\n",
      "Speed: 3.1ms preprocess, 18.7ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.8ms\n",
      "Speed: 2.1ms preprocess, 18.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.2ms\n",
      "Speed: 2.0ms preprocess, 18.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.4ms\n",
      "Speed: 2.1ms preprocess, 13.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.5ms\n",
      "Speed: 2.1ms preprocess, 15.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 22.3ms\n",
      "Speed: 2.6ms preprocess, 22.3ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.3ms\n",
      "Speed: 1.8ms preprocess, 17.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.8ms\n",
      "Speed: 2.1ms preprocess, 17.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.5ms\n",
      "Speed: 1.9ms preprocess, 17.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.3ms\n",
      "Speed: 2.5ms preprocess, 18.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.1ms\n",
      "Speed: 1.9ms preprocess, 13.1ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.3ms\n",
      "Speed: 1.7ms preprocess, 13.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.9ms\n",
      "Speed: 2.3ms preprocess, 12.9ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 34.1ms\n",
      "Speed: 7.5ms preprocess, 34.1ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.9ms\n",
      "Speed: 1.5ms preprocess, 12.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.2ms\n",
      "Speed: 1.1ms preprocess, 13.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 12.7ms\n",
      "Speed: 2.0ms preprocess, 12.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 13.2ms\n",
      "Speed: 2.2ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 14.0ms\n",
      "Speed: 1.3ms preprocess, 14.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 12.1ms\n",
      "Speed: 1.8ms preprocess, 12.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 13.4ms\n",
      "Speed: 1.5ms preprocess, 13.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 11.6ms\n",
      "Speed: 1.1ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 27.3ms\n",
      "Speed: 1.8ms preprocess, 27.3ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 12.4ms\n",
      "Speed: 1.3ms preprocess, 12.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 13.7ms\n",
      "Speed: 1.8ms preprocess, 13.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 12.9ms\n",
      "Speed: 1.1ms preprocess, 12.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 12.0ms\n",
      "Speed: 1.4ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 12.8ms\n",
      "Speed: 1.4ms preprocess, 12.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 12.3ms\n",
      "Speed: 1.6ms preprocess, 12.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 11.6ms\n",
      "Speed: 1.2ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 11.9ms\n",
      "Speed: 1.4ms preprocess, 11.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 21.7ms\n",
      "Speed: 1.6ms preprocess, 21.7ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 12.9ms\n",
      "Speed: 1.8ms preprocess, 12.9ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 13.1ms\n",
      "Speed: 1.7ms preprocess, 13.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 11.8ms\n",
      "Speed: 1.6ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 12.0ms\n",
      "Speed: 1.5ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 13.0ms\n",
      "Speed: 1.5ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 15.1ms\n",
      "Speed: 2.1ms preprocess, 15.1ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 13.1ms\n",
      "Speed: 1.4ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.4ms\n",
      "Speed: 1.4ms preprocess, 11.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.2ms\n",
      "Speed: 2.3ms preprocess, 12.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 24.3ms\n",
      "Speed: 4.5ms preprocess, 24.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.4ms\n",
      "Speed: 1.8ms preprocess, 12.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.4ms\n",
      "Speed: 1.5ms preprocess, 11.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.0ms\n",
      "Speed: 1.7ms preprocess, 12.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.9ms\n",
      "Speed: 2.3ms preprocess, 16.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.0ms\n",
      "Speed: 1.7ms preprocess, 14.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.5ms\n",
      "Speed: 3.3ms preprocess, 13.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.9ms\n",
      "Speed: 1.6ms preprocess, 12.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.8ms\n",
      "Speed: 2.3ms preprocess, 13.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.2ms\n",
      "Speed: 2.0ms preprocess, 13.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.7ms\n",
      "Speed: 1.6ms preprocess, 12.7ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.1ms\n",
      "Speed: 2.9ms preprocess, 14.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.7ms\n",
      "Speed: 1.9ms preprocess, 12.7ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.8ms\n",
      "Speed: 1.7ms preprocess, 12.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.7ms\n",
      "Speed: 1.7ms preprocess, 12.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.7ms\n",
      "Speed: 2.2ms preprocess, 12.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.6ms\n",
      "Speed: 1.7ms preprocess, 13.6ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.9ms\n",
      "Speed: 1.8ms preprocess, 15.9ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 24.1ms\n",
      "Speed: 2.7ms preprocess, 24.1ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.1ms\n",
      "Speed: 1.9ms preprocess, 16.1ms inference, 4.6ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import numpy as np\n",
    "import hog as hg\n",
    "import faceAlign as face\n",
    "import time\n",
    "\n",
    "# Reduce frames size to increase performance\n",
    "def resize_frame(frame, scale=0.5):\n",
    "    width = int(frame.shape[1] * scale)\n",
    "    height = int(frame.shape[0] * scale)\n",
    "    dimensions = (width, height)\n",
    "    return cv2.resize(frame, dimensions, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def reload_known_faces_list(path):\n",
    "    hog_list = []\n",
    "    name_list = []\n",
    "    # check if path exists, if not then make path\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Path {path} does not exist, creating path now\")\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # create encoding and name from files in the path\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(('.jpg', '.png')):\n",
    "            image_path = os.path.join(path, filename)\n",
    "\n",
    "            # align face from path\n",
    "            try:\n",
    "                face_frame = face.align_face_from_file(image_path)\n",
    "            except Exception as e:\n",
    "                print(f'Error with {filename}')\n",
    "\n",
    "            # calculate hog of aligned face frame\n",
    "            image_hog, _ = hg.calculate_hog_from_face_frame(face_frame) # dont need visualizations\n",
    "\n",
    "            hog_list.append(image_hog)\n",
    "            name = os.path.splitext(filename)[0]\n",
    "            name_list.append(name)\n",
    "            \n",
    "    return hog_list, name_list\n",
    "\n",
    "def dict_found_face(name, xyxy, score):\n",
    "    #returns a dictionary with all the arguments passed in\n",
    "    dict = {}\n",
    "    x1, y1, x2, y2 = map(int, xyxy)\n",
    "    dict['x1'] = x1\n",
    "    dict['y1'] = y1\n",
    "    dict['x2'] = x2\n",
    "    dict['y2'] = y2\n",
    "    dict['name'] = name\n",
    "    dict['score'] = score\n",
    "    return dict\n",
    "\n",
    "\n",
    "whitelist_path = r\"C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\known_faces\\whitelist\"\n",
    "blacklist_path = r\"C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\known_faces\\blacklist\"\n",
    "\n",
    "# initialize face library, apppend \"face\" class folder\n",
    "whitelist_hogs, whitelist_names = reload_known_faces_list(whitelist_path + r\"\\face\")\n",
    "blacklist_hogs, blacklist_names = reload_known_faces_list(blacklist_path + r\"\\face\")\n",
    "\n",
    "# Load the YOLO11 model\n",
    "model = YOLO(\"yolo11n_FaceDetectModel_MD20.pt\")\n",
    "\n",
    "# Open the video file/stream\n",
    "#video_path = r\"C:\\Users\\Derrick\\Videos\\\"\n",
    "cap = cv2.VideoCapture(0) # use cv2.VideoCapture(0) for live webcam\n",
    "\n",
    "# init some global variables\n",
    "distance_method = \"cosine\" # 'euclidean', 'cosine', or 'correlation'\n",
    "distance_threshold = 0.17\n",
    "found_dict = {}   #dictonary {'name': string, 'x1': int, 'y1': int, 'x2': int, 'y2': int}\n",
    "found_bool = False\n",
    "interval = 5  # Call face recog every 5 seconds\n",
    "next_call_time = time.time() + interval\n",
    "\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or processing is complete.\")\n",
    "        break\n",
    "\n",
    "    if success:\n",
    "        # Resize frame\n",
    "        #frame = resize_frame(frame, scale=0.5)\n",
    "\n",
    "        cv2.putText(frame,f'Method: {distance_method}', (10,400), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0),2)\n",
    "        cv2.putText(frame,f'Threshhold: {distance_threshold:.2f}', (10,425), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0),2)\n",
    "\n",
    "        # check if time interval has passed\n",
    "        current_time = time.time()\n",
    "        if current_time >= next_call_time:\n",
    "            found_bool = False\n",
    "            next_call_time = current_time + interval\n",
    "\n",
    "        # display bounding box and name of found face\n",
    "        try:\n",
    "            if found_bool:\n",
    "                cv2.rectangle(frame, (found_dict['x1'] - 10, found_dict['y1'] - 10), (found_dict['x2'] + 10, found_dict['y2'] + 10), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f'{found_dict[\"name\"]}', (found_dict['x1'], found_dict['y1'] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                cv2.putText(frame,f'Score: {found_dict[\"score\"]:.2f}', (10,450), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0),2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in display box: {e}\")\n",
    "        # Run YOLO11 tracking on the frame, persisting tracks between frames\n",
    "        #results = model.track(frame, persist=True)\n",
    "        #results = model.track(frame, persist=True, tracker=\"botsort.yaml\", conf=0.4, iou=0.5) #botsort tracker=\"bytetrack.yaml\"\n",
    "        results = model.predict(frame, conf=0.7)\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                if int(box.cls) == 0 and not found_bool:\n",
    "                    # put frame through face aligner\n",
    "                    face_frame = face.align_face_from_frame(frame)\n",
    "\n",
    "                    # calculate single hog from frame\n",
    "                    frame_hog, _ = hg.calculate_hog_from_face_frame(face_frame) # dont need visualizations\n",
    "                    # reset variables\n",
    "                    whitelist_dist = []\n",
    "                    blacklist_dist = []\n",
    "                    name = \"unknown\"\n",
    "\n",
    "                    # if face imgs exist in directory then do comparison of each face with detected face\n",
    "                    if whitelist_hogs is not None and whitelist_hogs:\n",
    "                        for hog in whitelist_hogs:\n",
    "                            whitelist_dist.append(hg.compare_hog_features(hog, frame_hog, distance_method))\n",
    "\n",
    "                    if blacklist_hogs is not None and blacklist_hogs:\n",
    "                        for hog in blacklist_hogs:\n",
    "                            blacklist_dist.append(hg.compare_hog_features(hog, frame_hog, distance_method))\n",
    "\n",
    "                    try:\n",
    "                        # check blacklist for any matches first\n",
    "                        if (len(blacklist_dist) > 0): #check if not empty\n",
    "                            lowest_score = np.min(blacklist_dist)\n",
    "                            # if score is below threshhold then assign blacklist face\n",
    "                            if lowest_score < distance_threshold:\n",
    "                                best_index = np.argmin(blacklist_dist)\n",
    "                                name = blacklist_names[best_index]\n",
    "                                \n",
    "                        # then check whitelist for matches\n",
    "                        if (len(whitelist_dist) > 0): #check if not empty\n",
    "                            lowest_score = np.min(whitelist_dist)\n",
    "                            # if score is below threshhold then assign whitelist face\n",
    "                            if lowest_score < distance_threshold:\n",
    "                                best_index = np.argmin(whitelist_dist)\n",
    "                                name = whitelist_names[best_index]\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in lists: {e}\")\n",
    "\n",
    "                    # finally default to unknown face if no matches in either black or white list\n",
    "                    found_bool = True\n",
    "                    found_dict = dict_found_face(name, results[0].boxes.xyxy[0], lowest_score)\n",
    "                    \n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        #annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"YOLO11 Inference\", frame)\n",
    "\n",
    "        # if 'c' is pressed change distance method\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"c\"):\n",
    "            #'euclidean', 'cosine', or 'correlation'\n",
    "            if distance_method == \"cosine\":\n",
    "                distance_method = \"correlation\"\n",
    "                distance_threshold = 0.45\n",
    "\n",
    "            elif distance_method == \"correlation\":\n",
    "                distance_method = \"euclidean\"\n",
    "                distance_threshold = 7\n",
    "            else:\n",
    "                distance_method = \"cosine\"\n",
    "                distance_threshold = 0.17\n",
    "\n",
    "        # if 'z' is pressed decrease threshold\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"z\"):\n",
    "            distance_threshold -= 0.1\n",
    "\n",
    "        # if 'x' is pressed increase threshold\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"x\"):\n",
    "            distance_threshold += 0.1\n",
    "\n",
    "        # if 'r' is pressed reload faces directory\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"r\"):\n",
    "            whitelist_encodes, whitelist_names = reload_known_faces_list(whitelist_path + r\"\\face\")\n",
    "            blacklist_encodes, blacklist_names = reload_known_faces_list(blacklist_path + r\"\\face\")\n",
    "\n",
    "        # if 'f' is pressed use save_crop to save bounding box image under folder class, in specified directory starting with string\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"f\"):\n",
    "            results[0].save_crop(whitelist_path, r\"whitelist_\")\n",
    "            print(f\"Whitelisted face to {whitelist_path}.\")\n",
    "\n",
    "        # if 'k' is pressed use save_crop to save bounding box image under folder class, in specified directory starting with string\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"k\"):\n",
    "            results[0].save_crop(blacklist_path, r\"blacklist_\")\n",
    "            print(f\"Blacklisted face to {blacklist_path}.\")\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc53654",
   "metadata": {},
   "source": [
    "<h1> Checkpoint 2, before face alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2918fe7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 13.8ms\n",
      "Speed: 2.3ms preprocess, 13.8ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 22.4ms\n",
      "Speed: 2.1ms preprocess, 22.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 34.1ms\n",
      "Speed: 8.9ms preprocess, 34.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.1ms\n",
      "Speed: 1.2ms preprocess, 17.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 36.1ms\n",
      "Speed: 4.6ms preprocess, 36.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 42.7ms\n",
      "Speed: 2.9ms preprocess, 42.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 35.0ms\n",
      "Speed: 2.4ms preprocess, 35.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 31.4ms\n",
      "Speed: 4.2ms preprocess, 31.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 36.7ms\n",
      "Speed: 4.2ms preprocess, 36.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 41.3ms\n",
      "Speed: 4.5ms preprocess, 41.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.9ms\n",
      "Speed: 3.1ms preprocess, 19.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.2ms\n",
      "Speed: 1.1ms preprocess, 12.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.0ms\n",
      "Speed: 1.2ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.8ms\n",
      "Speed: 1.3ms preprocess, 12.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.3ms\n",
      "Speed: 1.2ms preprocess, 12.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.1ms\n",
      "Speed: 1.2ms preprocess, 12.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.4ms\n",
      "Speed: 1.2ms preprocess, 12.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.3ms\n",
      "Speed: 1.5ms preprocess, 13.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.4ms\n",
      "Speed: 1.3ms preprocess, 12.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.9ms\n",
      "Speed: 1.2ms preprocess, 11.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.6ms\n",
      "Speed: 1.8ms preprocess, 13.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.0ms\n",
      "Speed: 1.1ms preprocess, 12.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.4ms\n",
      "Speed: 1.5ms preprocess, 12.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.8ms\n",
      "Speed: 1.5ms preprocess, 17.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.2ms\n",
      "Speed: 1.7ms preprocess, 15.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.1ms\n",
      "Speed: 1.3ms preprocess, 12.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.9ms\n",
      "Speed: 1.3ms preprocess, 16.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.1ms\n",
      "Speed: 1.4ms preprocess, 12.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.7ms\n",
      "Speed: 1.9ms preprocess, 12.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.4ms\n",
      "Speed: 1.6ms preprocess, 13.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.5ms\n",
      "Speed: 1.4ms preprocess, 14.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.1ms\n",
      "Speed: 1.4ms preprocess, 12.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.0ms\n",
      "Speed: 1.8ms preprocess, 12.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.4ms\n",
      "Speed: 1.3ms preprocess, 12.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.6ms\n",
      "Speed: 1.4ms preprocess, 13.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.2ms\n",
      "Speed: 2.4ms preprocess, 12.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.5ms\n",
      "Speed: 2.4ms preprocess, 14.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.8ms\n",
      "Speed: 1.6ms preprocess, 12.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.3ms\n",
      "Speed: 1.6ms preprocess, 15.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.0ms\n",
      "Speed: 2.0ms preprocess, 12.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.9ms\n",
      "Speed: 1.7ms preprocess, 11.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.0ms\n",
      "Speed: 1.6ms preprocess, 12.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.3ms\n",
      "Speed: 1.5ms preprocess, 12.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.1ms\n",
      "Speed: 1.7ms preprocess, 12.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.3ms\n",
      "Speed: 1.5ms preprocess, 12.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.1ms\n",
      "Speed: 1.8ms preprocess, 13.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.1ms\n",
      "Speed: 1.6ms preprocess, 15.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.6ms\n",
      "Speed: 1.6ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.6ms\n",
      "Speed: 1.6ms preprocess, 11.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.5ms\n",
      "Speed: 2.5ms preprocess, 12.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.5ms\n",
      "Speed: 1.7ms preprocess, 11.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.7ms\n",
      "Speed: 3.3ms preprocess, 19.7ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.1ms\n",
      "Speed: 4.2ms preprocess, 18.1ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.8ms\n",
      "Speed: 2.5ms preprocess, 16.8ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.9ms\n",
      "Speed: 3.4ms preprocess, 15.9ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.2ms\n",
      "Speed: 2.3ms preprocess, 16.2ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.9ms\n",
      "Speed: 3.1ms preprocess, 18.9ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.3ms\n",
      "Speed: 1.8ms preprocess, 18.3ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.7ms\n",
      "Speed: 1.8ms preprocess, 15.7ms inference, 6.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.9ms\n",
      "Speed: 1.8ms preprocess, 17.9ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.7ms\n",
      "Speed: 2.4ms preprocess, 13.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.1ms\n",
      "Speed: 2.5ms preprocess, 18.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.4ms\n",
      "Speed: 1.8ms preprocess, 18.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.8ms\n",
      "Speed: 2.9ms preprocess, 13.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.4ms\n",
      "Speed: 2.2ms preprocess, 18.4ms inference, 6.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 20.3ms\n",
      "Speed: 3.6ms preprocess, 20.3ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.8ms\n",
      "Speed: 1.8ms preprocess, 16.8ms inference, 5.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.2ms\n",
      "Speed: 3.3ms preprocess, 19.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.6ms\n",
      "Speed: 1.7ms preprocess, 16.6ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.4ms\n",
      "Speed: 2.8ms preprocess, 19.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.6ms\n",
      "Speed: 1.7ms preprocess, 16.6ms inference, 5.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.1ms\n",
      "Speed: 1.9ms preprocess, 17.1ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.9ms\n",
      "Speed: 1.8ms preprocess, 17.9ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.1ms\n",
      "Speed: 1.9ms preprocess, 18.1ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.9ms\n",
      "Speed: 2.1ms preprocess, 17.9ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.7ms\n",
      "Speed: 2.0ms preprocess, 16.7ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.5ms\n",
      "Speed: 2.2ms preprocess, 16.5ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.9ms\n",
      "Speed: 2.3ms preprocess, 15.9ms inference, 5.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.4ms\n",
      "Speed: 1.9ms preprocess, 16.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.7ms\n",
      "Speed: 2.2ms preprocess, 15.7ms inference, 5.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.6ms\n",
      "Speed: 1.9ms preprocess, 17.6ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 25.4ms\n",
      "Speed: 2.1ms preprocess, 25.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.8ms\n",
      "Speed: 3.2ms preprocess, 19.8ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.5ms\n",
      "Speed: 2.6ms preprocess, 17.5ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.8ms\n",
      "Speed: 2.8ms preprocess, 18.8ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 25.3ms\n",
      "Speed: 4.6ms preprocess, 25.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 25.5ms\n",
      "Speed: 4.9ms preprocess, 25.5ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 20.7ms\n",
      "Speed: 2.1ms preprocess, 20.7ms inference, 4.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.6ms\n",
      "Speed: 2.3ms preprocess, 19.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.3ms\n",
      "Speed: 1.8ms preprocess, 17.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 23.9ms\n",
      "Speed: 2.8ms preprocess, 23.9ms inference, 6.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 21.7ms\n",
      "Speed: 2.4ms preprocess, 21.7ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.9ms\n",
      "Speed: 2.2ms preprocess, 15.9ms inference, 5.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.9ms\n",
      "Speed: 2.1ms preprocess, 15.9ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.9ms\n",
      "Speed: 1.8ms preprocess, 18.9ms inference, 6.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.7ms\n",
      "Speed: 2.9ms preprocess, 18.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 17.9ms\n",
      "Speed: 2.3ms preprocess, 17.9ms inference, 5.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.5ms\n",
      "Speed: 2.1ms preprocess, 16.5ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 16.4ms\n",
      "Speed: 2.1ms preprocess, 16.4ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 15.6ms\n",
      "Speed: 2.0ms preprocess, 15.6ms inference, 7.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 16.0ms\n",
      "Speed: 2.2ms preprocess, 16.0ms inference, 8.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 15.6ms\n",
      "Speed: 2.4ms preprocess, 15.6ms inference, 10.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 17.3ms\n",
      "Speed: 3.7ms preprocess, 17.3ms inference, 5.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 21.3ms\n",
      "Speed: 1.9ms preprocess, 21.3ms inference, 7.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.6ms\n",
      "Speed: 2.7ms preprocess, 16.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 21.6ms\n",
      "Speed: 2.6ms preprocess, 21.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.9ms\n",
      "Speed: 3.0ms preprocess, 19.9ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.9ms\n",
      "Speed: 1.7ms preprocess, 15.9ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.1ms\n",
      "Speed: 3.3ms preprocess, 16.1ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.3ms\n",
      "Speed: 3.0ms preprocess, 16.3ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.4ms\n",
      "Speed: 3.7ms preprocess, 16.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.2ms\n",
      "Speed: 4.4ms preprocess, 16.2ms inference, 5.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.8ms\n",
      "Speed: 2.3ms preprocess, 17.8ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 27.3ms\n",
      "Speed: 3.3ms preprocess, 27.3ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 24.9ms\n",
      "Speed: 2.2ms preprocess, 24.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.9ms\n",
      "Speed: 2.2ms preprocess, 15.9ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 20.1ms\n",
      "Speed: 2.4ms preprocess, 20.1ms inference, 4.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.5ms\n",
      "Speed: 2.2ms preprocess, 17.5ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 20.6ms\n",
      "Speed: 2.1ms preprocess, 20.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.7ms\n",
      "Speed: 2.7ms preprocess, 15.7ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.0ms\n",
      "Speed: 2.5ms preprocess, 16.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.1ms\n",
      "Speed: 2.6ms preprocess, 17.1ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.9ms\n",
      "Speed: 2.9ms preprocess, 16.9ms inference, 4.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.7ms\n",
      "Speed: 2.1ms preprocess, 19.7ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 20.1ms\n",
      "Speed: 2.7ms preprocess, 20.1ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 23.8ms\n",
      "Speed: 2.8ms preprocess, 23.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.6ms\n",
      "Speed: 2.4ms preprocess, 17.6ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.0ms\n",
      "Speed: 2.9ms preprocess, 16.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 19.8ms\n",
      "Speed: 2.2ms preprocess, 19.8ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 21.2ms\n",
      "Speed: 2.1ms preprocess, 21.2ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 25.3ms\n",
      "Speed: 4.6ms preprocess, 25.3ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 20.1ms\n",
      "Speed: 2.3ms preprocess, 20.1ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 9.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 17.2ms\n",
      "Speed: 2.1ms preprocess, 17.2ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 17.7ms\n",
      "Speed: 3.7ms preprocess, 17.7ms inference, 5.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 16.5ms\n",
      "Speed: 1.9ms preprocess, 16.5ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 15.9ms\n",
      "Speed: 2.2ms preprocess, 15.9ms inference, 7.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 15.4ms\n",
      "Speed: 2.1ms preprocess, 15.4ms inference, 7.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 15.7ms\n",
      "Speed: 1.8ms preprocess, 15.7ms inference, 9.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 22.3ms\n",
      "Speed: 2.9ms preprocess, 22.3ms inference, 6.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 19.7ms\n",
      "Speed: 2.0ms preprocess, 19.7ms inference, 5.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 16.3ms\n",
      "Speed: 3.0ms preprocess, 16.3ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.2ms\n",
      "Speed: 3.1ms preprocess, 16.2ms inference, 4.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.0ms\n",
      "Speed: 2.6ms preprocess, 17.0ms inference, 6.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.9ms\n",
      "Speed: 4.5ms preprocess, 18.9ms inference, 5.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.1ms\n",
      "Speed: 2.7ms preprocess, 16.1ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 24.4ms\n",
      "Speed: 2.3ms preprocess, 24.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 18.2ms\n",
      "Speed: 2.0ms preprocess, 18.2ms inference, 7.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 17.2ms\n",
      "Speed: 2.4ms preprocess, 17.2ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 22.8ms\n",
      "Speed: 3.0ms preprocess, 22.8ms inference, 4.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 21.1ms\n",
      "Speed: 2.1ms preprocess, 21.1ms inference, 9.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 17.6ms\n",
      "Speed: 1.7ms preprocess, 17.6ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 16.7ms\n",
      "Speed: 2.0ms preprocess, 16.7ms inference, 10.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 21.0ms\n",
      "Speed: 2.3ms preprocess, 21.0ms inference, 8.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 17.4ms\n",
      "Speed: 2.1ms preprocess, 17.4ms inference, 5.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 16.3ms\n",
      "Speed: 2.0ms preprocess, 16.3ms inference, 8.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 face, 15.9ms\n",
      "Speed: 3.0ms preprocess, 15.9ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.3ms\n",
      "Speed: 2.0ms preprocess, 16.3ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.7ms\n",
      "Speed: 2.1ms preprocess, 16.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 22.9ms\n",
      "Speed: 2.1ms preprocess, 22.9ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.1ms\n",
      "Speed: 3.9ms preprocess, 17.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 20.3ms\n",
      "Speed: 3.4ms preprocess, 20.3ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.5ms\n",
      "Speed: 2.0ms preprocess, 16.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.5ms\n",
      "Speed: 2.5ms preprocess, 17.5ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 17.0ms\n",
      "Speed: 2.3ms preprocess, 17.0ms inference, 3.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 27.6ms\n",
      "Speed: 2.6ms preprocess, 27.6ms inference, 6.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 23.0ms\n",
      "Speed: 2.6ms preprocess, 23.0ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.1ms\n",
      "Speed: 3.7ms preprocess, 13.1ms inference, 7.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 18.6ms\n",
      "Speed: 1.8ms preprocess, 18.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.8ms\n",
      "Speed: 2.4ms preprocess, 16.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.7ms\n",
      "Speed: 2.3ms preprocess, 16.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.3ms\n",
      "Speed: 1.9ms preprocess, 14.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.9ms\n",
      "Speed: 2.2ms preprocess, 12.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 136.3ms\n",
      "Speed: 61.3ms preprocess, 136.3ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 39.6ms\n",
      "Speed: 4.7ms preprocess, 39.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 24.6ms\n",
      "Speed: 1.5ms preprocess, 24.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.6ms\n",
      "Speed: 1.6ms preprocess, 12.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 29.7ms\n",
      "Speed: 2.6ms preprocess, 29.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.6ms\n",
      "Speed: 1.4ms preprocess, 12.6ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.5ms\n",
      "Speed: 1.3ms preprocess, 12.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.3ms\n",
      "Speed: 1.1ms preprocess, 12.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 23.8ms\n",
      "Speed: 1.5ms preprocess, 23.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.0ms\n",
      "Speed: 1.1ms preprocess, 12.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.3ms\n",
      "Speed: 1.8ms preprocess, 15.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.0ms\n",
      "Speed: 1.1ms preprocess, 12.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.9ms\n",
      "Speed: 1.2ms preprocess, 11.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.4ms\n",
      "Speed: 2.3ms preprocess, 14.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 21.9ms\n",
      "Speed: 1.6ms preprocess, 21.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.7ms\n",
      "Speed: 1.2ms preprocess, 11.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.9ms\n",
      "Speed: 2.2ms preprocess, 15.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.1ms\n",
      "Speed: 1.6ms preprocess, 12.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.0ms\n",
      "Speed: 1.6ms preprocess, 14.0ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.8ms\n",
      "Speed: 3.6ms preprocess, 11.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.9ms\n",
      "Speed: 1.9ms preprocess, 13.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.7ms\n",
      "Speed: 1.4ms preprocess, 12.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.6ms\n",
      "Speed: 2.0ms preprocess, 15.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 16.9ms\n",
      "Speed: 1.8ms preprocess, 16.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.7ms\n",
      "Speed: 1.2ms preprocess, 11.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.5ms\n",
      "Speed: 1.9ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.2ms\n",
      "Speed: 1.2ms preprocess, 13.2ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.6ms\n",
      "Speed: 1.4ms preprocess, 11.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.0ms\n",
      "Speed: 1.4ms preprocess, 12.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 14.6ms\n",
      "Speed: 1.5ms preprocess, 14.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 12.2ms\n",
      "Speed: 1.4ms preprocess, 12.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 21.4ms\n",
      "Speed: 1.6ms preprocess, 21.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.3ms\n",
      "Speed: 2.2ms preprocess, 13.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 13.1ms\n",
      "Speed: 1.6ms preprocess, 13.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 15.3ms\n",
      "Speed: 2.4ms preprocess, 15.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.3ms\n",
      "Speed: 1.6ms preprocess, 11.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 11.7ms\n",
      "Speed: 1.7ms preprocess, 11.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import numpy as np\n",
    "import hog as hg\n",
    "import time\n",
    "\n",
    "# Reduce frames size to increase performance\n",
    "def resize_frame(frame, scale=0.5):\n",
    "    width = int(frame.shape[1] * scale)\n",
    "    height = int(frame.shape[0] * scale)\n",
    "    dimensions = (width, height)\n",
    "    return cv2.resize(frame, dimensions, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def reload_known_faces_list(path):\n",
    "    hog_list = []\n",
    "    name_list = []\n",
    "    # check if path exists, if not then make path\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Path {path} does not exist, creating path now\")\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # create encoding and name from files in the path\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(('.jpg', '.png')):\n",
    "            image_path = os.path.join(path, filename)\n",
    "            # align face from path\n",
    "            \n",
    "            # calculate hog of aligned face frame\n",
    "            image_hog, _ = hg.calculate_hog_from_path(image_path) # dont need visualizations\n",
    "\n",
    "            #face_enc = face_recognition.face_encodings(image)\n",
    "            \n",
    "            hog_list.append(image_hog)\n",
    "            name = os.path.splitext(filename)[0]\n",
    "            name_list.append(name)\n",
    "            \n",
    "    return hog_list, name_list\n",
    "\n",
    "def dict_found_face(name, xyxy, score):\n",
    "    #returns a dictionary with all the arguments passed in\n",
    "    dict = {}\n",
    "    x1, y1, x2, y2 = map(int, xyxy)\n",
    "    dict['x1'] = x1\n",
    "    dict['y1'] = y1\n",
    "    dict['x2'] = x2\n",
    "    dict['y2'] = y2\n",
    "    dict['name'] = name\n",
    "    dict['score'] = score\n",
    "    return dict\n",
    "\n",
    "\n",
    "\n",
    "whitelist_path = r\"C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\known_faces\\whitelist\"\n",
    "blacklist_path = r\"C:\\Users\\Derrick\\Documents\\School\\Computer Vision\\project\\known_faces\\blacklist\"\n",
    "\n",
    "# initialize face library, apppend \"face\" class folder\n",
    "whitelist_hogs, whitelist_names = reload_known_faces_list(whitelist_path + r\"\\face\")\n",
    "blacklist_hogs, blacklist_names = reload_known_faces_list(blacklist_path + r\"\\face\")\n",
    "\n",
    "# Load the YOLO11 model\n",
    "model = YOLO(\"yolo11n_FaceDetectModel_MD.pt\")\n",
    "\n",
    "# Open the video file/stream\n",
    "#video_path = r\"C:\\Users\\Derrick\\Videos\\\"\n",
    "cap = cv2.VideoCapture(0) # use cv2.VideoCapture(0) for live webcam\n",
    "\n",
    "# init some global variables\n",
    "distance_method = \"cosine\" # 'euclidean', 'cosine', or 'correlation'\n",
    "distance_threshold = 0.3 #    euc = 7\n",
    "found_dict = {}   #dictonary {'name': string, 'x1': int, 'y1': int, 'x2': int, 'y2': int}\n",
    "found_bool = False\n",
    "interval = 1  # Call my_function() every 1 seconds\n",
    "next_call_time = time.time() + interval\n",
    "\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or processing is complete.\")\n",
    "        break\n",
    "\n",
    "    if success:\n",
    "        # Resize frame\n",
    "        #frame = resize_frame(frame, scale=0.5)\n",
    "\n",
    "        cv2.putText(frame,f'Method: {distance_method}', (10,400), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0),2)\n",
    "        cv2.putText(frame,f'Threshhold: {distance_threshold:.2f}', (10,425), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0),2)\n",
    "\n",
    "        # check if time interval has passed\n",
    "        current_time = time.time()\n",
    "        if current_time >= next_call_time:\n",
    "            found_bool = False\n",
    "            next_call_time = current_time + interval\n",
    "\n",
    "        # display bounding box and name of found face \n",
    "        if found_bool:\n",
    "            cv2.rectangle(frame, (found_dict['x1'] - 10, found_dict['y1'] - 10), (found_dict['x2'] + 10, found_dict['y2'] + 10), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'{found_dict[\"name\"]}', (found_dict['x1'], found_dict['y1'] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            cv2.putText(frame,f'Score: {found_dict[\"score\"]:.2f}', (10,450), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0),2)\n",
    "\n",
    "        # Run YOLO11 tracking on the frame, persisting tracks between frames\n",
    "        results = model.predict(frame, conf=0.7)\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                if int(box.cls) == 0 and not found_bool:\n",
    "                    # put frame through face aligner\n",
    "                    # start here\n",
    "                    # calculate single hog from frame\n",
    "                    frame_hog, _ = hg.calculate_hog_from_frame(frame, results[0].boxes.xywh[0]) # dont need visualizations\n",
    "                    # reset variables\n",
    "                    whitelist_dist = []\n",
    "                    blacklist_dist = []\n",
    "                    name = \"unknown\"\n",
    "\n",
    "                    # if face imgs exist in directory then do comparison of each face with detected face\n",
    "                    if whitelist_hogs is not None and whitelist_hogs:\n",
    "                        for hog in whitelist_hogs:\n",
    "                            whitelist_dist.append(hg.compare_hog_features(hog, frame_hog, distance_method))\n",
    "\n",
    "                    if blacklist_hogs is not None and blacklist_hogs:\n",
    "                        for hog in blacklist_hogs:\n",
    "                            blacklist_dist.append(hg.compare_hog_features(hog, frame_hog, distance_method))\n",
    "\n",
    "                    \n",
    "                    # check blacklist for any matches first\n",
    "                    if (len(blacklist_dist) > 0): #check if not empty\n",
    "                        lowest_score = np.min(blacklist_dist)\n",
    "                        # if score is below threshhold then assign blacklist face\n",
    "                        if lowest_score < distance_threshold:\n",
    "                            best_index = np.argmin(blacklist_dist)\n",
    "                            name = blacklist_names[best_index]\n",
    "                            \n",
    "                    # then check whitelist for matches\n",
    "                    if (len(whitelist_dist) > 0): #check if not empty\n",
    "                        lowest_score = np.min(whitelist_dist)\n",
    "                        # if score is below threshhold then assign whitelist face\n",
    "                        if lowest_score < distance_threshold:\n",
    "                            best_index = np.argmin(whitelist_dist)\n",
    "                            name = whitelist_names[best_index]\n",
    "\n",
    "                    # finally default to unknown face if no matches in either black or white list\n",
    "                    found_bool = True\n",
    "                    found_dict = dict_found_face(name, results[0].boxes.xyxy[0], lowest_score)\n",
    "                    \n",
    "        #bug defaults to single image in black list even with match in whitelist\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        #annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"YOLO11 Inference\", frame)\n",
    "\n",
    "        # if 'c' is pressed change distance method\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"c\"):\n",
    "            #'euclidean', 'cosine', or 'correlation'\n",
    "            if distance_method == \"cosine\":\n",
    "                distance_method = \"correlation\"\n",
    "                distance_threshold = 1.1\n",
    "\n",
    "            elif distance_method == \"correlation\":\n",
    "                distance_method = \"euclidean\"\n",
    "                distance_threshold = 8.5\n",
    "            else:\n",
    "                distance_method = \"cosine\"\n",
    "                distance_threshold = 0.2\n",
    "\n",
    "        # if 'z' is pressed decrease threshold\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"z\"):\n",
    "            distance_threshold -= 0.1\n",
    "\n",
    "        # if 'x' is pressed increase threshold\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"x\"):\n",
    "            distance_threshold += 0.1\n",
    "\n",
    "        # if 'r' is pressed reload faces directory\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"r\"):\n",
    "            whitelist_encodes, whitelist_names = reload_known_faces_list(whitelist_path + r\"\\face\")\n",
    "            blacklist_encodes, blacklist_names = reload_known_faces_list(blacklist_path + r\"\\face\")\n",
    "\n",
    "        # if 'f' is pressed use save_crop to save bounding box image under folder class, in specified directory starting with string\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"f\"):\n",
    "            results[0].save_crop(whitelist_path, r\"whitelist_\")\n",
    "            print(f\"Whitelisted face to {whitelist_path}.\")\n",
    "\n",
    "        # if 'k' is pressed use save_crop to save bounding box image under folder class, in specified directory starting with string\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"k\"):\n",
    "            results[0].save_crop(blacklist_path, r\"blacklist_\")\n",
    "            print(f\"Blacklisted face to {blacklist_path}.\")\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
